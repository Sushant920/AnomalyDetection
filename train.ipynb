{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.17.0\n",
      "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices(\"GPU\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Loading data...\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "print(\"🔄 Loading data...\")\n",
    "train_data = pd.read_csv(\"data/TEP_FaultFree_Training.csv\")\n",
    "test_data = pd.read_csv(\"data/TEP_FaultFree_Testing.csv\")\n",
    "\n",
    "# Drop index column if necessary\n",
    "train_data = train_data.iloc[:, 1:]\n",
    "test_data = test_data.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "scaler = MinMaxScaler()\n",
    "train_scaled = scaler.fit_transform(train_data)\n",
    "test_scaled = scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = train_scaled.shape[1]\n",
    "input_layer = keras.Input(shape=(input_dim,))\n",
    "encoded = layers.Dense(128, activation='relu')(input_layer)\n",
    "encoded = layers.Dense(64, activation='relu')(encoded)\n",
    "encoded = layers.Dense(32, activation='relu', activity_regularizer=regularizers.l1(1e-4))(encoded)\n",
    "\n",
    "decoded = layers.Dense(64, activation='relu')(encoded)\n",
    "decoded = layers.Dense(128, activation='relu')(decoded)\n",
    "decoded = layers.Dense(input_dim, activation='sigmoid')(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training autoencoder...\n",
      "Epoch 1/50\n",
      "\u001b[1m7813/7813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 15ms/step - loss: 0.0161 - val_loss: 0.0243\n",
      "Epoch 2/50\n",
      "\u001b[1m7813/7813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 14ms/step - loss: 0.0160 - val_loss: 0.0243\n",
      "Epoch 3/50\n",
      "\u001b[1m7813/7813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 15ms/step - loss: 0.0160 - val_loss: 0.0243\n",
      "Epoch 4/50\n",
      "\u001b[1m7813/7813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 16ms/step - loss: 0.0160 - val_loss: 0.0243\n",
      "Epoch 5/50\n",
      "\u001b[1m7813/7813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 16ms/step - loss: 0.0160 - val_loss: 0.0243\n",
      "Epoch 6/50\n",
      "\u001b[1m7813/7813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 15ms/step - loss: 0.0160 - val_loss: 0.0243\n",
      "Epoch 7/50\n",
      "\u001b[1m7813/7813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 17ms/step - loss: 0.0160 - val_loss: 0.0242\n",
      "Epoch 8/50\n",
      "\u001b[1m7813/7813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 17ms/step - loss: 0.0160 - val_loss: 0.0243\n",
      "Epoch 9/50\n",
      "\u001b[1m7813/7813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 18ms/step - loss: 0.0160 - val_loss: 0.0242\n",
      "Epoch 10/50\n",
      "\u001b[1m7813/7813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 17ms/step - loss: 0.0160 - val_loss: 0.0242\n",
      "Epoch 11/50\n",
      "\u001b[1m7813/7813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 16ms/step - loss: 0.0160 - val_loss: 0.0242\n",
      "Epoch 12/50\n",
      "\u001b[1m7813/7813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 15ms/step - loss: 0.0160 - val_loss: 0.0243\n",
      "Epoch 13/50\n",
      "\u001b[1m7813/7813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 15ms/step - loss: 0.0160 - val_loss: 0.0243\n",
      "Epoch 14/50\n",
      "\u001b[1m7813/7813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 15ms/step - loss: 0.0160 - val_loss: 0.0243\n",
      "Epoch 15/50\n",
      "\u001b[1m7813/7813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 16ms/step - loss: 0.0160 - val_loss: 0.0242\n",
      "Epoch 16/50\n",
      "\u001b[1m7813/7813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 14ms/step - loss: 0.0160 - val_loss: 0.0243\n",
      "Epoch 17/50\n",
      "\u001b[1m7813/7813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 15ms/step - loss: 0.0160 - val_loss: 0.0242\n",
      "Epoch 18/50\n",
      "\u001b[1m7813/7813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 15ms/step - loss: 0.0160 - val_loss: 0.0243\n",
      "Epoch 19/50\n",
      "\u001b[1m7813/7813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 16ms/step - loss: 0.0160 - val_loss: 0.0243\n",
      "Epoch 20/50\n",
      "\u001b[1m7813/7813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 15ms/step - loss: 0.0160 - val_loss: 0.0243\n",
      "Epoch 21/50\n",
      "\u001b[1m7813/7813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 15ms/step - loss: 0.0160 - val_loss: 0.0243\n",
      "Epoch 22/50\n",
      "\u001b[1m7813/7813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 15ms/step - loss: 0.0160 - val_loss: 0.0242\n",
      "Epoch 23/50\n",
      "\u001b[1m7813/7813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 14ms/step - loss: 0.0160 - val_loss: 0.0242\n",
      "Epoch 24/50\n",
      "\u001b[1m7813/7813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 14ms/step - loss: 0.0160 - val_loss: 0.0242\n",
      "Epoch 25/50\n",
      "\u001b[1m7813/7813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 14ms/step - loss: 0.0160 - val_loss: 0.0243\n",
      "Epoch 26/50\n",
      "\u001b[1m7813/7813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 14ms/step - loss: 0.0160 - val_loss: 0.0242\n",
      "Epoch 27/50\n",
      "\u001b[1m7813/7813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 14ms/step - loss: 0.0160 - val_loss: 0.0243\n",
      "Epoch 28/50\n",
      "\u001b[1m7813/7813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 14ms/step - loss: 0.0160 - val_loss: 0.0243\n",
      "Epoch 29/50\n",
      "\u001b[1m7813/7813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 14ms/step - loss: 0.0160 - val_loss: 0.0242\n",
      "Epoch 30/50\n",
      "\u001b[1m7813/7813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 14ms/step - loss: 0.0160 - val_loss: 0.0243\n",
      "Epoch 31/50\n",
      "\u001b[1m7813/7813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m239s\u001b[0m 31ms/step - loss: 0.0160 - val_loss: 0.0243\n",
      "Epoch 32/50\n",
      "\u001b[1m7813/7813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 14ms/step - loss: 0.0160 - val_loss: 0.0243\n",
      "Epoch 33/50\n",
      "\u001b[1m7813/7813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 15ms/step - loss: 0.0160 - val_loss: 0.0243\n",
      "Epoch 34/50\n",
      "\u001b[1m7813/7813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 15ms/step - loss: 0.0160 - val_loss: 0.0242\n",
      "Epoch 35/50\n",
      "\u001b[1m7813/7813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 15ms/step - loss: 0.0160 - val_loss: 0.0243\n",
      "Epoch 36/50\n",
      "\u001b[1m7813/7813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 15ms/step - loss: 0.0160 - val_loss: 0.0243\n",
      "Epoch 37/50\n",
      "\u001b[1m7809/7813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0160"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "autoencoder = keras.Model(input_layer, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Encoder model\n",
    "encoder = keras.Model(input_layer, encoded)\n",
    "\n",
    "# Train autoencoder\n",
    "print(\"🚀 Training autoencoder...\")\n",
    "history = autoencoder.fit(\n",
    "    train_scaled, train_scaled,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(test_scaled, test_scaled),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and scaler\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "autoencoder.save(\"models/autoencoder_dip.h5\")\n",
    "np.save(\"models/scaler.npy\", scaler)\n",
    "print(\"✅ Model trained and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract latent space representation\n",
    "latent_rep = encoder.predict(test_scaled)\n",
    "\n",
    "# t-SNE visualization\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_results = tsne.fit_transform(latent_rep)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(tsne_results[:, 0], tsne_results[:, 1], alpha=0.5)\n",
    "plt.title('t-SNE of Latent Space')\n",
    "plt.show()\n",
    "\n",
    "# Reconstruction error analysis\n",
    "reconstructions = autoencoder.predict(test_scaled)\n",
    "recon_errors = np.mean(np.square(test_scaled - reconstructions), axis=1)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(recon_errors, kde=True, stat='probability')\n",
    "plt.axvline(np.percentile(recon_errors, 95), color='r', linestyle='--')  # 95% threshold\n",
    "plt.title('Reconstruction Error Distribution')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
